set dotenv-load := true
set dotenv-filename := "ENV"
set shell := ["bash", "-cu"]

export ARG := " --custom_config_base null --custom_config_version null --pipelines_testdata_base_path null"

# Print pipeline name
data:
    @set -euo pipefail; \
    echo $PIPELINE

# Inspect S3 dataset and local offline artifacts
check_data:
    @set -euo pipefail; \
    : "${PIPELINE:?set PIPELINE in ENV}"; \
    : "${S3_ROOT:?set S3_ROOT in ENV or shell}"; \
    S3_PIPE_ROOT="${S3_ROOT}/${PIPELINE}"; \
    echo "== S3 path =="; echo "${S3_PIPE_ROOT}/data"; \
    echo "== S3 objects =="; aws s3 ls "${S3_PIPE_ROOT}/data" --recursive || true; \
    echo; echo "== offline/inputs3.csv =="; test -f offline/inputs3.csv && cat offline/inputs3.csv || echo "missing"; \
    echo; echo "== offline/offline_test.conf =="; test -f offline/offline_test.conf && sed -n '1,120p' offline/offline_test.conf || echo "missing"

# Mirror samplesheet (input) from tests config to S3 and create offline/inputs3.csv
data_input:
    @set -euo pipefail; \
    : "${PIPELINE:?set PIPELINE in ENV}"; \
    : "${S3_ROOT:?set S3_ROOT in ENV or shell}"; \
    bash ../../common/data/mirror_testdata.sh --rows 1 --param-name input --conf ./conf/test.config --pipeline "$PIPELINE"

# Mirror references (fasta, gtf) to S3
data_refs:
    @set -euo pipefail; \
    : "${PIPELINE:?set PIPELINE in ENV}"; \
    : "${S3_ROOT:?set S3_ROOT in ENV or shell}"; \
    bash ../../common/data/mirror_param_url.sh --param-name fasta --conf ./conf/test.config --pipeline "$PIPELINE"; \
    bash ../../common/data/mirror_param_url.sh --param-name gtf   --conf ./conf/test.config --pipeline "$PIPELINE"

# Convenience: run both input+refs mirroring
data:
    just data_input data_refs

# Verify config has S3 URIs for key params
verify_offline:
    @set -euo pipefail; \
    rg -n "^[[:space:]]*(params\.)?(input|fasta|gtf)[[:space:]]*=[[:space:]]*'s3://" conf/test.config || { echo "[x] Missing S3 URIs for one or more params (input/fasta/gtf)"; exit 1; }

# Convenience: run mirroring then show data
prep:
    just data check_data

# Online run (uses quay registry override)
test:
    nextflow run . -profile test -resume -c <(echo 'docker { registry = "quay.io" }') -w /tmp/work-$PIPELINE $ARG

# Online stub run
stub:
    nextflow run . -profile test -resume -c <(echo 'docker { registry = "quay.io" }') -w /tmp/work-$PIPELINE -stub-run $ARG

# Online preview
preview:
    nextflow run . -profile test -resume -c <(echo 'docker { registry = "quay.io" }') -w /tmp/work-$PIPELINE -preview $ARG

# Upload code/config to S3 (excludes work/)
up:
    @set -euo pipefail; \
    : "${PIPELINE:?set PIPELINE in ENV}"; \
    : "${S3_ROOT:?set S3_ROOT in ENV or shell}"; \
    aws s3 sync "${HOME}/offline/${PIPELINE}/${PIPELINE}" "${S3_ROOT}/${PIPELINE}/${PIPELINE}" --follow-symlinks --exclude ".nextflow/*" --delete

# Download code/config from S3 (excludes work/)
down:
    @set -euo pipefail; \
    : "${PIPELINE:?set PIPELINE in ENV}"; \
    : "${ROOT_DIR:?set ROOT_DIR in ENV or shell}"; \
    : "${S3_ROOT:?set S3_ROOT in ENV or shell}"; \
    aws s3 sync "${S3_ROOT}/$PIPELINE/$PIPELINE" "${ROOT_DIR}/$PIPELINE/$PIPELINE" --follow-symlinks --exclude ".nextflow/*" --delete

# Offline full run
run:
    nextflow run . -profile test -offline -resume -w ${HOME}/work-$PIPELINE $ARG

# Offline stub run
stub2:
    nextflow run . -profile test -offline -resume -w ${HOME}/work-$PIPELINE -stub-run $ARG

# Offline preview
preview2:
    nextflow run . -profile test -offline -resume -w ${HOME}/work-$PIPELINE -preview $ARG

# chmod +x -c /home/ssm-user/offline/rnaseq/rnaseq/bin/*
execpermission:
    chmod +x -c bin/*

# Convenience groups
online:
    just test up

offline:
    just down run

# Local cleanup
clean:
    nextflow clean -f; rm -rf /tmp/nxf-work/ .nextflow null

## (Plugins and Quay helper recipes removed by request; use simple shell commands instead.)

# Verify environment (NXF_VER, PIPELINE, REVISION, S3/ROOT, NXF paths)
verify_env:
    @set -euo pipefail; \
    echo "PIPELINE=${PIPELINE:-}"; \
    echo "REVISION=${REVISION:-}"; \
    echo "S3_ROOT=${S3_ROOT:-}"; \
    echo "ROOT_DIR=${ROOT_DIR:-}"; \
    echo "NXF_HOME=${NXF_HOME:-$HOME/.nextflow}"; \
    echo "NXF_WORK=${NXF_WORK:-}"; \
    echo "NXF_VER=${NXF_VER:-(unset)}"; \
    nextflow -version || true

# Verify test.config symlink
verify_config:
    @set -euo pipefail; \
    if [ -L conf/test.config ]; then \
      echo "[✔] conf/test.config is symlink -> $$(readlink conf/test.config)"; \
    elif [ -f conf/test.config ]; then \
      echo "[i] conf/test.config is a regular file (upstream) — OK before finalize"; \
    else \
      echo "[x] conf/test.config missing"; exit 1; \
    fi

# Promote upstream conf/test.config to root test.config and link back
finalize_config:
    @set -euo pipefail; \
    STAGE_DIR="$PWD"; \
    ROOT_DIR="$PWD/.."; \
    TEST_STAGE="$STAGE_DIR/conf/test.config"; \
    [[ -f "$TEST_STAGE" ]] || { echo "ERROR: $TEST_STAGE not a regular file (already linked or missing)"; exit 2; }; \
    TS="$$(date +%Y%m%d%H%M%S)"; \
    BK="/tmp/$${PIPELINE:-pipeline}-conf.test.config.$$TS"; \
    echo "[i] Backup: $$TEST_STAGE -> $$BK"; \
    cp -a "$$TEST_STAGE" "$$BK"; \
    echo "[i] Remove stage conf/test.config"; \
    rm -f "$$TEST_STAGE"; \
    DEST_ROOT="$$ROOT_DIR/test.config"; \
    if [ -e "$$DEST_ROOT" ]; then \
      echo "[i] Root test.config exists; backing up"; \
      cp -a "$$DEST_ROOT" "$$DEST_ROOT.bak-$$TS"; \
    fi; \
    echo "[i] Promote to root: $$BK -> $$DEST_ROOT"; \
    cp -a "$$BK" "$$DEST_ROOT"; \
    echo "[i] Link back: conf/test.config -> $$DEST_ROOT"; \
    ln -sv "$$DEST_ROOT" "$$TEST_STAGE"; \
    echo "[✔] Finalized: conf/test.config now links to root test.config"
