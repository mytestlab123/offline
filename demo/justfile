set dotenv-load := true
set dotenv-filename := ".env"

# Derive pipeline name from this repo's folder
#PIPELINE := "{{file_name(justfile_directory())}}"
PIPELINE := "demo"

# Prepare S3 inputs + offline config (keeps repo light)
data:
    echo $PIPELINE

# Inspect created artifacts
check_data:
    @set -euo pipefail; \
    : "${S3_ROOT:?set S3_ROOT in .env or env}"; \
    S3_PIPE_ROOT="${S3_ROOT}/{{PIPELINE}}"; \
    echo "== S3 path =="; echo "${S3_PIPE_ROOT}/data"; \
    echo "== S3 objects =="; aws s3 ls "${S3_PIPE_ROOT}/data" --recursive || true; \
    echo; echo "== offline/inputs3.csv =="; test -f offline/inputs3.csv && cat offline/inputs3.csv || echo "missing"; \
    echo; echo "== offline/offline_test.conf =="; test -f offline/offline_test.conf && sed -n '1,120p' offline/offline_test.conf || echo "missing"

# Quick online run against offline config (uses Nexus proxy)
test:
    nextflow run . -profile test -resume -c <(echo 'docker { registry = "quay.io" }') -w /tmp/work-rnaseq -stub

# Quick online run against offline config (uses Nexus proxy)
preview:
    nextflow run . -profile test -resume -c <(echo 'docker { registry = "quay.io" }') -w /tmp/work-rnaseq -preview

# Push this repo to S3 (code only; exclude heavy dirs)
push:
    just clean
    aws s3 sync "${HOME}/offline/${PIPELINE}/${PIPELINE}" "${S3_ROOT}/${PIPELINE}/${PIPELINE}"  --delete

# Pull mirror inside air-gapped VPC (offline side)
pull:
    just clean
    aws s3 sync "${S3_ROOT}/{{PIPELINE}}/{{PIPELINE}}" "${ROOT_DIR}/{{PIPELINE}}/{{PIPELINE}}" --delete

# Fully offline run
run:
    nextflow run . -profile test -offline -resume

# Stub run
stub:
    nextflow run . -profile test -offline -resume -w ${HOME}/work-demo -stub

preview2:
    nextflow run . -profile test -resume  -w /tmp/work-rnaseq -preview

# Convenience groups
online:
    just test push

offline:
    just pull run

# Local cleanup
clean:
    nextflow clean -f; rm -rf  /tmp/nxf-work/ .nextflow null

