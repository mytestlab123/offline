set dotenv-load := true
set dotenv-filename := ".env"

# Derive pipeline name from this repo's folder
#PIPELINE := "{{file_name(justfile_directory())}}"
PIPELINE := "demo"

# Prepare S3 inputs + offline config (keeps repo light)
data:
    echo $PIPELINE

# Inspect created artifacts
check_data:
    @set -euo pipefail; \
    : "${S3_ROOT:?set S3_ROOT in .env or env}"; \
    S3_PIPE_ROOT="${S3_ROOT}/{{PIPELINE}}"; \
    echo "== S3 path =="; echo "${S3_PIPE_ROOT}/data"; \
    echo "== S3 objects =="; aws s3 ls "${S3_PIPE_ROOT}/data" --recursive || true; \
    echo; echo "== offline/inputs3.csv =="; test -f offline/inputs3.csv && cat offline/inputs3.csv || echo "missing"; \
    echo; echo "== offline/offline_test.conf =="; test -f offline/offline_test.conf && sed -n '1,120p' offline/offline_test.conf || echo "missing"

# Quick online run against offline config (uses Nexus proxy)
test:
    nextflow run . -profile test -resume -c <(echo 'docker { registry = "quay.io" }') -w /tmp/work-rnaseq -stub

# Quick online run against offline config (uses Nexus proxy)
preview:
    nextflow run . -profile test -resume -c <(echo 'docker { registry = "quay.io" }') -w /tmp/work-rnaseq -preview

# Push this repo to S3 (code only; exclude heavy dirs)
push:
    just clean
    aws s3 sync "${HOME}/offline/${PIPELINE}/${PIPELINE}" "${S3_ROOT}/${PIPELINE}/${PIPELINE}"  --delete

# Pull mirror inside air-gapped VPC (offline side)
pull:
    just clean
    aws s3 sync "${S3_ROOT}/{{PIPELINE}}/{{PIPELINE}}" "${ROOT_DIR}/{{PIPELINE}}/{{PIPELINE}}" --delete

# Fully offline run
run:
    nextflow run . -profile test -offline -resume

# Stub run
stub:
    nextflow run . -profile test -offline -resume -w ${HOME}/work-demo -stub

preview2:
    nextflow run . -profile test -resume  -w /tmp/work-rnaseq -preview

# Convenience groups
online:
    just test push

offline:
    just pull run

# Local cleanup
clean:
    nextflow clean -f; rm -rf  /tmp/nxf-work/ .nextflow

set dotenv-load := true
set dotenv-filename := ".env"

# Derive pipeline name from this repo's folder
PIPELINE := "{{file_name(justfile_directory())}}"

# Prepare S3 inputs + offline config (keeps repo light)
data:
    ./offline/prepare_offline_test.sh

# Inspect created artifacts
check_data:
    @set -euo pipefail; \
    : "${S3_ROOT:?set S3_ROOT in .env or env}"; \
    S3_PIPE_ROOT="${S3_ROOT}/{{PIPELINE}}"; \
    echo "== S3 path =="; echo "${S3_PIPE_ROOT}/data"; \
    echo "== S3 objects =="; aws s3 ls "${S3_PIPE_ROOT}/data" --recursive || true; \
    echo; echo "== offline/inputs3.csv =="; test -f offline/inputs3.csv && cat offline/inputs3.csv || echo "missing"; \
    echo; echo "== offline/offline_test.conf =="; test -f offline/offline_test.conf && sed -n '1,120p' offline/offline_test.conf || echo "missing"

# Quick online run against offline config (uses Nexus proxy)
test_online:
    time nextflow run . -profile podman -c ./offline/offline_test.conf -resume -c ./offline/process_fix.conf

# Push this repo to S3 (code only; exclude heavy dirs)
s3_push:
    nextflow clean -f; export PIPELINE=demo; echo ${PIPELINE}; 
    aws s3 sync "${HOME}/offline/${PIPELINE}/${PIPELINE}" "${S3_ROOT}/${PIPELINE}/${PIPELINE}"  

# Pull mirror inside air-gapped VPC (offline side)
s3_pull:
    sudo rm -vrf .nextflow/* work null *.swp ".nextflow/" nxf-tmp* || true
    nextflow clean -f; rm -vrf work null .nextflow || true
    aws s3 sync "${S3_ROOT}/{{PIPELINE}}/{{PIPELINE}}" "${ROOT_DIR}/{{PIPELINE}}/{{PIPELINE}}" --delete

# Fully offline run
run:
    @export NXF_OFFLINE=true NXF_PLUGIN_AUTOINSTALL=false; \
    time sudo -E nextflow run . -profile podman -c ./offline/offline_test.conf -offline --outdir /tmp/out -resume --custom_config_base '' -c ./offline/process_fix.conf

# Convenience groups
online:
    just data check_data test_online s3_push

offline:
    just s3_pull run

# Local cleanup
clean:
    rm -rf .nextflow logs .nf* work null || true
